{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test 2, Question 51\n",
    "> **Hint:** In Databricks, import code for all questions via this URL:\n",
    "> \n",
    "> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./create_itemsDf2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b5d9e73-5701-49b6-9573-85eb622a6934",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DOC\n",
    "The size operator and the length operator can easily be confused. size works on arrays, while length works on strings. Luckily, this is something you can read up about in the documentation.\n",
    "\n",
    "### pyspark.sql.functions.size(col)\n",
    "\n",
    "  Collection function: returns the length of the array or map stored in the column.\n",
    "  \n",
    "### pyspark.sql.functions.length(col)\n",
    "\n",
    "  Computes the character length of string data or number of bytes of binary data. \n",
    "\n",
    "Built-in\n",
    "\n",
    "### len(s)\n",
    "  Return the length (the number of items) of an object. The argument may be a sequence (such as a string, bytes, tuple, list, or range) or a collection (such as a dictionary, set, or frozen set).\n",
    "  \n",
    "\n",
    "### pyspark.sql.functions.regexp_extract(str, pattern, idx)\n",
    "Extract a specific group matched by a Java regex, from the specified string column. If the regex did not match, or the specified group did not match, an empty string is returned. Index starts with 1.\n",
    "   \n",
    "### pyspark.sql.functions.regexp_replace(str, pattern, replacement)\n",
    "Replace all substrings of the specified string value that match regexp with rep.\n",
    "   \n",
    "###  pyspark.sql.functions.lower(col)\n",
    "Converts a string expression to lower case.\n",
    "\n",
    "\n",
    "\n",
    "### DF.select(*cols)\n",
    "Projects a set of expressions and returns a new DataFrame.\n",
    "\n",
    "Parameters\n",
    "\n",
    "cols – list of column names (string) or expressions (Column). If one of the column names is ‘*’, that column is expanded to include all columns in the current DataFrame.\n",
    "\n",
    "\n",
    "\n",
    "### DF.selectExpr(*expr)\n",
    "Projects a set of SQL expressions and returns a new DataFrame.\n",
    "This is a variant of select() that accepts SQL expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 1 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, upper, col, length\n",
    "\n",
    "itemsDf.select(length(regexp_extract(upper(col(\"itemName\")), \"a|e|i|o|u|\\s\", \"\")).as(\"consonant_ct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 2 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, size\n",
    "\n",
    "itemsDf.select(size(regexp_replace(lower(\"itemName\"), \"a|e|i|o|u|\\s\", \"\")).alias(\"consonant_ct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 3 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, col, length\n",
    "\n",
    "# itemsDf.select(lower(regexp_replace(length(\"itemName\"), \"a|e|i|o|u|\\s\", \"\")).alias(\"consonant_ct\")).show()\n",
    "# itemsDf.select(regexp_replace(length(\"itemName\"), \"a|e|i|o|u|\\s\", \"\")).alias(\"consonant_ct\").show()\n",
    "# itemsDf.select(length(\"itemName\")).alias(\"consonant_ct\").show()\n",
    "itemsDf.select(lower(length(\"itemName\"))).alias(\"consonant_ct\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 4 (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7193aa7-5004-476d-a11a-15dd3833c449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace, lower, col, length\n",
    "\n",
    "itemsDf.select(length(regexp_replace(lower(col(\"itemName\")), \"a|e|i|o|u|\\s\", \"\")).alias(\"consonant_ct\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 5 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_extract, lower, col, size\n",
    "\n",
    "itemsDf.select(size(regexp_extract(lower(col(\"itemName\")), \"a|e|i|o|u|\\s\", \"\")).alias(\"consonant_ct\")).show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "51",
   "notebookOrigID": 3688993815073588,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
