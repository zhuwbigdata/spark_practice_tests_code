{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test 3, Question 19\n",
    "> **Hint:** In Databricks, import code for all questions via this URL:\n",
    "> \n",
    "> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "08b1fd05-cf2e-4cb0-ad4d-31db2d3979bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Spark Schema DDL types\n",
    "df.schema.toDDL\n",
    "\n",
    "https://spark.apache.org/docs/3.0.0-preview/sql-ref-datatypes.html\n",
    "https://vincent.doba.fr/posts/20211004_spark_data_description_language_for_defining_spark_schema/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./create_itemsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae50de9e-30ce-4c67-a0ae-bf60cf25999c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2907363c-bb01-4e0d-8c5f-87449955ced3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePath = '/FileStore/itemsDf.parquet'\n",
    "\n",
    "itemsDf.write.format(\"parquet\").mode(\"overwrite\").save(filePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 1 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDfSchema = StructType([\n",
    "  StructField(\"itemId\", IntegerType()),\n",
    "  StructField(\"attributes\", StringType()),\n",
    "  StructField(\"supplier\", StringType())])\n",
    "\n",
    "itemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 2 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDfSchema = StructType([\n",
    "  StructField(\"itemId\", IntegerType),\n",
    "  StructField(\"attributes\", ArrayType(StringType)),\n",
    "  StructField(\"supplier\", StringType)])\n",
    "\n",
    "itemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 3 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDf = spark.read.schema('itemId integer, attributes <string>, supplier string').parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9cf9a0b2-fea2-4913-9c45-516f65e66557",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDf = spark.read.schema('itemId integer, attributes array<string>, supplier string').parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 4 (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7193aa7-5004-476d-a11a-15dd3833c449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDfSchema = StructType([\n",
    "  StructField(\"itemId\", IntegerType()),\n",
    "  StructField(\"attributes\", ArrayType(StringType())),\n",
    "  StructField(\"supplier\", StringType())])\n",
    "\n",
    "itemsDf = spark.read.schema(itemsDfSchema).parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 5 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField, IntegerType, ArrayType, StringType\n",
    "\n",
    "itemsDfSchema = StructType([\n",
    "  StructField(\"itemId\", IntegerType()),\n",
    "  StructField(\"attributes\", ArrayType([StringType()])),\n",
    "  StructField(\"supplier\", StringType())])\n",
    "\n",
    "itemsDf = spark.read(schema=itemsDfSchema).parquet(filePath)\n",
    "\n",
    "itemsDf.show(truncate=False)\n",
    "itemsDf.printSchema()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "19",
   "notebookOrigID": 3688993815074412,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
