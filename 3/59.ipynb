{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "87a4339f-cc26-4446-83b0-5fd419c20c70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Test 3, Question 59\n",
    "> **Hint:** In Databricks, import code for all questions via this URL:\n",
    "> \n",
    "> https://github.com/flrs/spark_practice_tests_code/raw/main/spark_practice_tests_code.dbc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55560add-b820-4f60-8684-117076e2fabd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### spark.read.options(**options)\n",
    "Adds input options for the underlying data source.\n",
    "\n",
    "Map map(a=>b, c=>d) or list(key=value)\n",
    "\n",
    "###  option(key, value)\n",
    "Adds an input option for the underlying data source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce0bf962-8af0-4c63-8bed-7acbfb428c56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./create_itemsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13c4e5c9-e615-48e2-9b8e-6a3baf42aa55",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "itemsDf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c0561dd-65fc-4777-a91d-0f7c0236583c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "filePathSave = '/DataStore/segment_723'\n",
    "filePath = filePathSave + '.parquet'\n",
    "itemsDf.repartition(1).write.mode('overwrite').parquet(filePathSave)\n",
    "\n",
    "file = dbutils.fs.ls(filePathSave)[-1].path\n",
    "dbutils.fs.cp(file, filePath)\n",
    "dbutils.fs.rm(filePathSave, recurse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8fef6132-53bf-4c40-b876-e17972c28fe8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 1 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4a1dfb7c-aaf8-4c46-89c0-7367cb77a413",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, ArrayType, StringType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructType(\"itemId\", IntegerType(), True),\n",
    "  StructType(\"attributes\", ArrayType([StringType(), True]), True),\n",
    "  StructType(\"supplier\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read().options(\"pathGlobFilter\", \"*_723.parquet\").schema(schema).parquet(\"/DataStore/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9edca3fe-7135-4cbe-a61a-a5dbd2d4c733",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 2 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6c3f712f-0536-4ce8-ad81-b36c5ec71945",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, ArrayType, StringType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"itemId\", IntegerType(), True),\n",
    "  StructField(\"attributes\", ArrayType(StringType(), True), True),\n",
    "  StructField(\"supplier\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read().options(\"pathGlobFilter\", \"*_723.parquet\").schema(schema).load(\"/DataStore/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3da08153-8385-45d8-85f4-9d1e5156ec37",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 3 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "741846ba-74a7-4878-9715-49e613b7b7e2",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "schema = StructField([\n",
    "  StructType(\"itemId\", IntegerType(), True),\n",
    "  StructType(\"attributes\", ArrayType(StringType(), True), True),\n",
    "  StructType(\"supplier\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read.options(pathGlobFilter=\"*_723.parquet\").schema(schema).load(\"/DataStore/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2f909ada-99cb-406f-8df0-0f31d52f554c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 4 (correct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7193aa7-5004-476d-a11a-15dd3833c449",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, ArrayType, StringType, StructField\n",
    "\n",
    "schema = StructType([\n",
    "  StructField(\"itemId\", IntegerType(), True),\n",
    "  StructField(\"attributes\", ArrayType(StringType(), True), True),\n",
    "  StructField(\"supplier\", StringType(), True)\n",
    "])\n",
    "\n",
    "spark.read.options(pathGlobFilter=\"*_723.parquet\").schema(schema).load(\"/DataStore/\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9e7b8a51-bd8b-45ce-aed6-488cc351332a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Answer 5 (incorrect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "14f9adb3-dc1d-4e18-8cfa-4fddecdfdfcd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, IntegerType, ArrayType, StringType\n",
    "\n",
    "schema = StructType([\n",
    "  StructType(\"itemId\", IntegerType(), False),\n",
    "  StructType(\"attributes\", ArrayType(StringType(), False), False),\n",
    "  StructType(\"supplier\", StringType(), False)\n",
    "])\n",
    "\n",
    "spark.read.options(pathGlobFilter=\"*_723.parquet\").schema(schema).load(filePath)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "59",
   "notebookOrigID": 3688993815073598,
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
